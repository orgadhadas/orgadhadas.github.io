<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Hadas Orgad</title>
        <link rel="icon" type="image/x-icon" href="assets/img/favicon.ico" />
        <!-- Font Awesome icons (free version)-->
        <script src="https://use.fontawesome.com/releases/v6.1.0/js/all.js" crossorigin="anonymous"></script>
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:500,700" rel="stylesheet" type="text/css" />
        <link href="https://fonts.googleapis.com/css?family=Muli:400,400i,800,800i" rel="stylesheet" type="text/css" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="css/styles.css" rel="stylesheet" />
		<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    </head>
    <body id="page-top">
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
            <a class="navbar-brand js-scroll-trigger" href="#page-top">
                <span class="d-block d-lg-none">Hadas Orgad</span>
                <span class="d-none d-lg-block"><img class="img-fluid img-profile rounded-circle mx-auto mb-2" src="assets/img/profile.jpg" alt="..." /></span>
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button>
            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav">
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#about">About</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#publications">Publications</a></li>
                    <!-- <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#education">Education</a></li> -->
                    <!-- <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#skills">Skills</a></li> -->
                    <!-- <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#interests">Interests</a></li> -->
                    <!-- <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#awards">Awards</a></li> -->
                </ul>
            </div>
        </nav>
        <!-- Page Content-->
		
        <div class="container-fluid p-0">
	
			
            <!-- About-->
            <section class="resume-section" id="about">
                <div class="resume-section-content">
                    <h1 class="mb-0">
                        Hadas Orgad
                    </h1>
					<p class="lead mb-5">I’m a Research Fellow at the <a href="https://kempnerinstitute.harvard.edu/">Kempner Institute for the Study of Natural and Artificial Intelligence</a> at Harvard University, where I study the internals of AI. My research explores how interpretability can be used as a strategic tool to improve their robustness, safety, and trustworthiness. I worked on problems related to hallucinations, bias, or unsafe outputs — with the broader goal of creating models that are both powerful and responsible.</p>
					<p class="lead mb-5"><b>Past:</b> I completed my Ph.D. at the <a href="https://cs.technion.ac.il/">Technion – Israel Institute of Technology</a>, supervised by <a href="https://belinkov.com/">Yonatan Belinkov</a>. Before that, I spent 3.5 years at Microsoft, where I worked on AI solutions for cloud security and on the application of NLP for security-related problems. I hold both my B.Sc. and M.Sc. degrees from the Technion. I was selected as a <a href="https://machinelearning.apple.com/updates/apple-scholars-aiml-2023">2023 Apple Scholar in AI/ML</a>, and previously interned at Apple. During my master’s, I received the 2022 EMEA Generation Google Scholarship.</p>
					<p class="lead mb-5">I’m always happy to connect with others who are excited about AI interpretability — feel free to reach out if you’d like to brainstorm or collaborate.</p>

					<div class="social-icons">
                        <a class="social-icon" href="https://www.linkedin.com/in/hadas-orgad/"><i class="fab fa-linkedin-in"></i></a>
                        <a class="social-icon" href="https://github.com/orgadhadas"><i class="fab fa-github"></i></a>
                        <a class="social-icon" href="https://twitter.com/OrgadHadas"><i class="fab fa-twitter"></i></a>
                        <a class="social-icon" href="https://scholar.google.com/citations?user=xWntyLkAAAA"><i class="ai ai-google-scholar"></i></a>
                    </div>
					<p class="lead mb-5" style="margin-top: 1rem;"> <img src="./assets/email_icon.webp" alt="email icon" width="45" height="50"> orgadhadas at gmail dot com </p>
                </div>
            </section>
            <hr class="m-0" />
			
			<!-- Publications -->
			
			<section class="resume-section" id="publications">
			
				<div class="resume-section-content">
                    <h2 class="mb-5">Publications</h2>
					<div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3>Inside-out: Hidden Factual Knowledge in LLMs</h3>
							<h4>Zorik Gekhman, Eyal Ben David, Hadas Orgad, Eran Ofek, Yonatan Belinkov, Idan Szpektor, Jonathan Herzig, Roi Reichart</h4>
                            <div class="subheading">COLM 2025</div>
                            <p>
							This work introduces a framework to measure "hidden knowledge" in large language models—cases where models internally know the correct answer but fail to express it in their outputs. By comparing internal and external knowledge across three LLMs, we find a consistent 40% gap, with some answers never generated despite perfect internal knowledge.
							</p>
							<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/zorikg/inside-out" target="_blank" rel="noopener">Code</a>
							<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/abs/2503.15299" target="_blank" rel="noopener">Arxiv</a>
							<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="./publications/inside_out/cite.bib" target="_blank" rel="noopener">Cite</a>
						</div>
						<div class="flex-shrink-0"><span class="text-primary">2025</span></div>
					</div>
					
					<div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3>LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations</h3>
							<h4>Hadas Orgad, Michael Toker, Zorik Gekhman, Roi Reichart, Idan Szpektor, Hadas Kotek, Yonatan Belinkov</h4>
                            <div class="subheading">ICLR 2025</div>
                            <p>
							This work shows that large language models internally encode rich information about the truthfulness of their outputs, concentrated in specific tokens—enabling strong error detection and even prediction of error types. However, this encoding is not universal across datasets. Additionally, models may still produce incorrect answers despite internally encoding the correct one.
							</p>
							<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://llms-know.github.io/" target="_blank" rel="noopener">Project Page</a>
							<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/technion-cs-nlp/LLMsKnow" target="_blank" rel="noopener">Code</a>
							<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/abs/2410.02707" target="_blank" rel="noopener">Arxiv</a>
							<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="./publications/llms_know/cite.bib" target="_blank" rel="noopener">Cite</a>
						</div>
						<div class="flex-shrink-0"><span class="text-primary">2025</span></div>
					</div>
								
					<div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3>MIB: A Mechanistic Interpretability Benchmark</h3>
							<h4>Aaron Mueller, Atticus Geiger, Sarah Wiegreffe, Dana Arad, Iván Arcuschin, Adam Belfki, Yik Siu Chan, Jaden Fiotto-Kaufman, Tal Haklay, Michael Hanna, Jing Huang, Rohan Gupta, Yaniv Nikankin, Hadas Orgad, Nikhil Prakash, Anja Reusch, Aruna Sankaranarayanan, Shun Shao, Alessandro Stolfo, Martin Tutek, Amir Zur, David Bau, Yonatan Belinkov</h4>
                            <div class="subheading">ICML 2025</div>
                            <p>
							A new benchmark designed to evaluate whether mechanistic interpretability methods truly improve our understanding of language models. MIB includes two tracks—circuit localization and causal variable identification—across multiple tasks and models.
							</p>
							<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://mib-bench.github.io/" target="_blank" rel="noopener">Project Page</a>
							<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/aaronmueller/MIB" target="_blank" rel="noopener">Code</a>
							<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/abs/2504.13151" target="_blank" rel="noopener">Arxiv</a>
							<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="./publications/MIB/cite.bib" target="_blank" rel="noopener">Cite</a>
						</div>
						<div class="flex-shrink-0"><span class="text-primary">2025</span></div>
					</div>
					
					<div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3>Padding Tone: A Mechanistic Analysis of Padding Tokens in T2I Models</h3>
							<h4>Michael Toker, Ido Galil, Hadas Orgad, Rinon Gal, Yoad Tewel, Gal Chechik, Yonatan Belinkov</h4>
                            <div class="subheading">NAACL 2025</div>
                            <p>
							This work presents the first in-depth analysis of how padding tokens affect text-to-image generation. Using two causal techniques, we find that padding tokens can influence image generation at different stages—during text encoding, the diffusion process, or not at all—depending on model architecture and training setup.
							</p>
							<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://padding-tone.github.io/" target="_blank" rel="noopener">Project Page</a>
							<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/technion-cs-nlp/Padding-Tone" target="_blank" rel="noopener">Code</a>
							<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/abs/2501.06751" target="_blank" rel="noopener">Arxiv</a>
							<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="./publications/padding_tone/cite.bib" target="_blank" rel="noopener">Cite</a>
						</div>
						<div class="flex-shrink-0"><span class="text-primary">2025</span></div>
					</div>
					
					
					<div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3>Position-Aware Automatic Circuit Discovery</h3>
							<h4>Tal Haklay, Hadas Orgad, David Bau, Aaron Mueller, Yonatan Belinkov</h4>
                            <div class="subheading">ACL 2025</div>
                            <p>
							We present an automated, position-aware circuit discovery pipeline that differentiates token positions and uses dataset schemas to capture cross-positional mechanisms with better faithfulness at smaller sizes.
							</p>
							<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://peap-circuits.github.io/" target="_blank" rel="noopener">Project Page</a>
							<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="http://github.com/technion-cs-nlp/PEAP" target="_blank" rel="noopener">Code</a>
							<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/abs/2502.04577" target="_blank" rel="noopener">Arxiv</a>
							<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="./publications/peap/cite.bib" target="_blank" rel="noopener">Cite</a>
						</div>
						<div class="flex-shrink-0"><span class="text-primary">2025</span></div>
					</div>
					
					<div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3>Diffusion Lens: Interpreting Text Encoders in Text-to-Image Pipelines</h3>
							<h4> Michael Toker, Hadas Orgad, Mor Ventura, Dana Arad, Yonatan Belinkov </h4>
                            <div class="subheading">ACL 2024</div>
                            <p>
							We introduce the Diffusion Lens method to analyze text encoders intermediate representations in text-to-image models. Using the method, we perform an extensive analysis of two recent T2I models, and gain insights on both conceptual combination abilities and knowledge retrieval in the models.
							</p>
							<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://tokeron.github.io/DiffusionLensWeb/" target="_blank" rel="noopener">Project Page</a>
							<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/tokeron/DiffusionLens" target="_blank" rel="noopener">Code</a>
							<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/abs/2403.05846" target="_blank" rel="noopener">Arxiv</a>
							<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="./publications/diffusion_lens/cite.bib" target="_blank" rel="noopener">Cite</a>
						</div>
						<div class="flex-shrink-0"><span class="text-primary">2024</span></div>
					</div>
					
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3>ReFACT: Updating Text-to-Image Models by Editing the Text Encoder</h3>
							<h4> Dana Arad*, Hadas Orgad*, Yonatan Belinkov </h4>
                            <div class="subheading">NAACL 2024</div>
                            <p>
							Text-to-image models are trained on extensive amounts of data, leading them to implicitly encode factual knowledge within their parameters. While some facts are useful, others may be incorrect or become outdated (e.g., the current President of the United States). We introduce ReFACT, a method for updating text-to-image models. ReFACT updates the weights of a specific layer in the text encoder, only modifying a tiny portion of the model's parameters, and leaving the rest of the model unaffected.</p>
							<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://technion-cs-nlp.github.io/ReFACT/" target="_blank" rel="noopener">Project Page</a>
							<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/technion-cs-nlp/ReFACT" target="_blank" rel="noopener">Code</a>
							<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/abs/2306.00738" target="_blank" rel="noopener">Arxiv</a>
							<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="./publications/ReFACT/cite.bib" target="_blank" rel="noopener">Cite</a>
						</div>
						<div class="flex-shrink-0"><span class="text-primary">2023</span></div>
					</div>
					
					<div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3>Unified Concept Editing in Diffusion Models</h3>
							<h4> Rohit Gandikota, Hadas Orgad, Yonatan Belinkov, Joanna Materzyńska, David Bau </h4>
                            <div class="subheading">IEEE/CVF WACV 2024</div>
                            <p>
							Text-to-image models suffer from various safety issues that may limit their suitability for deployment. Previous methods have separately addressed individual issues of bias, copyright, and offensive content in text-to-image models. However, in the real world, all of these issues appear simultaneously in the same model.
							In this paper, we present a method that tackles those diverse issues with a single approach.</p>
							<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://unified.baulab.info/" target="_blank" rel="noopener">Project Page</a>
							<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/rohitgandikota/unified-concept-editing" target="_blank" rel="noopener">Code</a>
							<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/abs/2308.14761" target="_blank" rel="noopener">Arxiv</a>
							<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="./publications/UCE/cite.bib" target="_blank" rel="noopener">Cite</a>
						</div>
						<div class="flex-shrink-0"><span class="text-primary">2023</span></div>
					</div>
					
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3>Editing Implicit Assumptions in Text-to-Image Diffusion Models</h3>
							<h4> Hadas Orgad*, Bahjat Kawar*, Yonatan Belinkov </h4>
                            <div class="subheading">ICCV 2023</div>
                            <p>
							Text-to-image diffusion models often make implicit assumptions about the world when generating images. While some assumptions are useful (e.g., the sky is blue), they can also be outdated, incorrect, or reflective of social biases present in the training data.
							In this work, we aim to edit a given implicit assumption in a pre-trained diffusion model.
							Our method is highly efficient, as it modifies a mere 2.2% of the model's parameters in under one second.
							</p>
							<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://time-diffusion.github.io/" target="_blank" rel="noopener">Project Page</a>
							<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/bahjat-kawar/time-diffusion" target="_blank" rel="noopener">Code</a>
							<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/abs/2303.08084" target="_blank" rel="noopener">Arxiv</a>
							<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="./publications/TIME/cite.bib" target="_blank" rel="noopener">Cite</a>
						</div>
						<div class="flex-shrink-0"><span class="text-primary">2023</span></div>
					</div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3>Debiasing NLP Models Without Demographic Information</h3>
							<h4> Hadas Orgad, Yonatan Belinkov </h4>
                            <div class="subheading">ACL 2023</div>
                            <p>
							In this work, we propose a debiasing method that operates without any prior knowledge of the demographics in the dataset, detecting biased examples based on an auxiliary model that predicts the main model's success and down-weights them during the training process. Results on racial and gender bias demonstrate that it is possible to mitigate social biases without having to use a costly demographic annotation process.
							</p>
							<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/technion-cs-nlp/BLIND" target="_blank" rel="noopener">Code</a>
							<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/abs/2212.10563" target="_blank" rel="noopener">Arxiv</a>
							<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="./publications/Debiasing-NLP-Models-Without-Demographic-Information/cite.bib" target="_blank" rel="noopener">Cite</a>
						</div>
						<div class="flex-shrink-0"><span class="text-primary">2022</span></div>
					</div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Choose your lenses: Flaws in gender bias evaluation</h3>
							<h4> Hadas Orgad, Yonatan Belinkov </h4>
                            <div class="subheading mb-3">GeBNLP 2022</div>
                            <p>Considerable efforts to measure and mitigate gender bias in recent years have led to the introduction of an abundance of tasks, datasets, and metrics used in this vein. In this position paper, we assess the current paradigm of gender bias evaluation and identify several flaws in it.</p>
                        	<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://aclanthology.org/2022.gebnlp-1.17/" target="_blank" rel="noopener">Arxiv</a>
							<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="./publications/Choose-Your-Lenses/cite.bib" target="_blank" rel="noopener">Cite</a>
						</div>
                        <div class="flex-shrink-0"><span class="text-primary">2022</span></div>
                    </div>
					<div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">How Gender Debiasing Affects Internal Model Representations, and Why It Matters</h3>
							<h4>Hadas Orgad, Seraphina Goldfarb-Tarrant, Yonatan Belinkov</h4>
                            <div class="subheading mb-3">NAACL 2022</div>
                            <p>Common studies of gender bias in NLP focus either on extrinsic bias measured by model performance on a downstream task or on intrinsic bias found in models’ internal representations. However, the relationship between extrinsic and intrinsic bias is relatively unknown. In this work, we illuminate this relationship by measuring both quantities together.</p>
                        	<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/technion-cs-nlp/gender_internal" target="_blank" rel="noopener">Code</a>
							<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://aclanthology.org/2022.naacl-main.188/" target="_blank" rel="noopener">Arxiv</a>
							<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="./publications/How-Gender-Debiasing-Affects-Internal-Model-Representations/cite.bib" target="_blank" rel="noopener">Cite</a>
						</div>
                        <div class="flex-shrink-0"><span class="text-primary">2022</span></div>
                    </div>
                </div>
            </section>
            <hr class="m-0" />
		
			
            
        <!-- Bootstrap core JS-->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
		<!-- Load jQuery, SimpleModal and Basic JS files -->
		<script type='text/javascript' src='js/jquery.js'></script>
		<script type='text/javascript' src='js/jquery.simplemodal.js'></script>
		<script type='text/javascript' src='js/basic.js'></script>
		
    </body>
</html>
